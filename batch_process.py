import os
import glob
import shutil
import sys
from pdf_process_tools.process import run_pdf_processing
from vector_utils import load_texts_from_output, build_faiss_index, model
import pickle
import numpy as np
import json

def batch_process_pdfs(input_dir, output_base_dir):
    """
    æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹å†…æ‰€æœ‰PDFæ–‡ä»¶
    
    Args:
        input_dir: åŒ…å«PDFæ–‡ä»¶çš„è¾“å…¥ç›®å½•
        output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•ï¼Œæ¯ä¸ªPDFä¼šåœ¨æ­¤ç›®å½•ä¸‹åˆ›å»ºå¯¹åº”çš„æ–‡ä»¶å¤¹
    """
    # åˆ›å»ºè¾“å‡ºåŸºç¡€ç›®å½•
    os.makedirs(output_base_dir, exist_ok=True)
    
    # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
    pdf_files = glob.glob(os.path.join(input_dir, "*.pdf"))
    
    if not pdf_files:
        print(f"âš ï¸ åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶ï¼Œå¼€å§‹æ‰¹é‡å¤„ç†...")
    
    # ç»Ÿè®¡å¤„ç†ç»“æœ
    success_count = 0
    failed_count = 0
    skipped_count = 0
    
    for i, pdf_path in enumerate(pdf_files, 1):
        pdf_filename = os.path.basename(pdf_path)
        pdf_name = os.path.splitext(pdf_filename)[0]  # å»æ‰.pdfæ‰©å±•å
        
        # ä¸ºæ¯ä¸ªPDFåˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
        doc_output_dir = os.path.join(output_base_dir, pdf_name)
        
        print(f"\nğŸ“„ [{i}/{len(pdf_files)}] æ­£åœ¨å¤„ç†: {pdf_filename}")
        print(f"   è¾“å‡ºç›®å½•: {doc_output_dir}")
        
        try:
            # å¦‚æœè¾“å‡ºç›®å½•å·²å­˜åœ¨ä¸”åŒ…å«å¤„ç†å®Œæˆçš„æ ‡å¿—ï¼Œè·³è¿‡å¤„ç†
            if is_already_processed(doc_output_dir):
                print(f"   â­ï¸ è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶: {pdf_filename}")
                skipped_count += 1
                continue
            
            # æ¸…ç†å¯èƒ½å­˜åœ¨çš„ä¸å®Œæ•´è¾“å‡ºç›®å½•
            if os.path.exists(doc_output_dir):
                shutil.rmtree(doc_output_dir)
            
            # è°ƒç”¨ pdf_processing.py ä¸­çš„ä¸»è¦å¤„ç†å‡½æ•°
            print(f"   ğŸ”„ å¼€å§‹å¤„ç†...")
            try:
                success = run_pdf_processing(pdf_path, doc_output_dir)
                print(f"   ğŸ“‹ run_pdf_processing è¿”å›å€¼: {success}")
                print(f"   ğŸ“ è¾“å‡ºç›®å½•æ˜¯å¦å­˜åœ¨: {os.path.exists(doc_output_dir)}")
                
                if os.path.exists(doc_output_dir):
                    files = os.listdir(doc_output_dir)
                    print(f"   ğŸ“„ è¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶: {files}")
                
            except Exception as proc_e:
                print(f"   âŒ run_pdf_processing æŠ›å‡ºå¼‚å¸¸: {proc_e}")
                success = False
            
            # éªŒè¯å¤„ç†ç»“æœ
            validation_result = validate_processing_result(doc_output_dir)
            print(f"   ğŸ” éªŒè¯ç»“æœ: {validation_result}")
            
            # ä»¥éªŒè¯ç»“æœä¸ºå‡†
            if validation_result:
                print(f"   âœ… æ ¹æ®éªŒè¯ç»“æœï¼Œå¤„ç†æˆåŠŸ: {pdf_filename}")
                success_count += 1
                try:
                    build_vector_index(doc_output_dir)
                except:
                    print(f"   âš ï¸ å‘é‡ç´¢å¼•å»ºç«‹å¤±è´¥ï¼Œä½†ä¸å½±å“ä¸»è¦å¤„ç†")
            else:
                print(f"   âŒ æ ¹æ®éªŒè¯ç»“æœï¼Œå¤„ç†å¤±è´¥: {pdf_filename}")
                failed_count += 1
            
        except Exception as e:
            print(f"   âŒ å¤„ç†å¼‚å¸¸: {pdf_filename}, é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            failed_count += 1
            # æ¸…ç†å¤±è´¥çš„è¾“å‡ºç›®å½•
            if os.path.exists(doc_output_dir):
                shutil.rmtree(doc_output_dir, ignore_errors=True)
            continue
    
    # è¾“å‡ºæ‰¹å¤„ç†ç»“æœç»Ÿè®¡
    print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"   âœ… æˆåŠŸ: {success_count}")
    print(f"   âŒ å¤±è´¥: {failed_count}")
    print(f"   â­ï¸ è·³è¿‡: {skipped_count}")
    print(f"   ğŸ“ ç»“æœä¿å­˜åœ¨: {output_base_dir}")
    
    # ç”Ÿæˆæ‰¹å¤„ç†æŠ¥å‘Š
    generate_batch_report(output_base_dir, success_count, failed_count, skipped_count)

def is_already_processed(doc_output_dir):
    """
    æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²ç»å¤„ç†å®Œæˆ
    
    Args:
        doc_output_dir: æ–‡æ¡£è¾“å‡ºç›®å½•
        
    Returns:
        bool: æ˜¯å¦å·²å¤„ç†å®Œæˆ
    """
    if not os.path.exists(doc_output_dir):
        return False
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = [
        "processing_report.json",
        "final_text.txt"
    ]
    
    for file in required_files:
        if not os.path.exists(os.path.join(doc_output_dir, file)):
            return False
    
    # æ£€æŸ¥å¤„ç†æŠ¥å‘Šä¸­çš„çŠ¶æ€
    try:
        report_path = os.path.join(doc_output_dir, "processing_report.json")
        with open(report_path, 'r', encoding='utf-8') as f:
            report = json.load(f)
        
        # æ£€æŸ¥å¤„ç†çŠ¶æ€
        return report.get('processing_status') in ['success', 'partial']
    except:
        return False

def validate_processing_result(doc_output_dir):
    """
    éªŒè¯å¤„ç†ç»“æœçš„å®Œæ•´æ€§
    
    Args:
        doc_output_dir: æ–‡æ¡£è¾“å‡ºç›®å½•
        
    Returns:
        bool: å¤„ç†ç»“æœæ˜¯å¦æœ‰æ•ˆ
    """
    # æ£€æŸ¥åŸºæœ¬æ–‡ä»¶
    if not os.path.exists(os.path.join(doc_output_dir, "processing_report.json")):
        return False
    
    # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€äº›æœ‰ç”¨çš„è¾“å‡º
    has_text = os.path.exists(os.path.join(doc_output_dir, "final_text.txt"))
    has_images = any(f.endswith('.png') for f in os.listdir(doc_output_dir))
    
    return has_text or has_images

def build_vector_index(doc_output_dir):
    """
    ä¸ºå•ä¸ªæ–‡æ¡£å»ºç«‹å‘é‡ç´¢å¼•
    
    Args:
        doc_output_dir: æ–‡æ¡£çš„è¾“å‡ºç›®å½•
    """
    print("   ğŸ” æ­£åœ¨å»ºç«‹å‘é‡ç´¢å¼•...")
    
    try:
        # åŠ è½½æ–‡æœ¬
        texts = load_texts_from_output(doc_output_dir)
        if not texts:
            print("   âš ï¸ æœªæ‰¾åˆ°æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡å‘é‡ç´¢å¼•å»ºç«‹")
            return
        
        # å»ºç«‹FAISSç´¢å¼•
        index, embeddings = build_faiss_index(texts)
        
        # ä¿å­˜ç´¢å¼•å’Œç›¸å…³æ•°æ®
        index_dir = os.path.join(doc_output_dir, "vector_index")
        os.makedirs(index_dir, exist_ok=True)
        
        # ä¿å­˜FAISSç´¢å¼•
        import faiss
        faiss.write_index(index, os.path.join(index_dir, "faiss.index"))
        
        # ä¿å­˜æ–‡æœ¬å’ŒåµŒå…¥å‘é‡
        with open(os.path.join(index_dir, "texts.pkl"), 'wb') as f:
            pickle.dump(texts, f)
        
        np.save(os.path.join(index_dir, "embeddings.npy"), embeddings)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "num_texts": len(texts),
            "embedding_dim": embeddings.shape[1],
            "model_path": getattr(model, 'model_name_or_path', "unknown"),
            "created_at": str(pd.Timestamp.now()) if 'pd' in globals() else "unknown"
        }
        
        with open(os.path.join(index_dir, "metadata.json"), 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"   âœ… å‘é‡ç´¢å¼•å·²ä¿å­˜: {len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
        
    except Exception as e:
        print(f"   âš ï¸ å‘é‡ç´¢å¼•å»ºç«‹å¤±è´¥: {e}")

def load_vector_index(doc_output_dir):
    """
    åŠ è½½å·²ä¿å­˜çš„å‘é‡ç´¢å¼•
    
    Args:
        doc_output_dir: æ–‡æ¡£çš„è¾“å‡ºç›®å½•
    
    Returns:
        tuple: (index, texts, embeddings) æˆ– None
    """
    index_dir = os.path.join(doc_output_dir, "vector_index")
    
    if not os.path.exists(index_dir):
        return None
    
    try:
        import faiss
        # åŠ è½½FAISSç´¢å¼•
        index = faiss.read_index(os.path.join(index_dir, "faiss.index"))
        
        # åŠ è½½æ–‡æœ¬
        with open(os.path.join(index_dir, "texts.pkl"), 'rb') as f:
            texts = pickle.load(f)
        
        # åŠ è½½åµŒå…¥å‘é‡
        embeddings = np.load(os.path.join(index_dir, "embeddings.npy"))
        
        return index, texts, embeddings
    
    except Exception as e:
        print(f"âš ï¸ åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {e}")
        return None

def generate_batch_report(output_base_dir, success_count, failed_count, skipped_count):
    """
    ç”Ÿæˆæ‰¹å¤„ç†æŠ¥å‘Š
    
    Args:
        output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        success_count: æˆåŠŸå¤„ç†çš„æ–‡ä»¶æ•°
        failed_count: å¤±è´¥çš„æ–‡ä»¶æ•°
        skipped_count: è·³è¿‡çš„æ–‡ä»¶æ•°
    """
    report = {
        "batch_processing_summary": {
            "total_files": success_count + failed_count + skipped_count,
            "successful": success_count,
            "failed": failed_count,
            "skipped": skipped_count,
            "success_rate": f"{success_count/(success_count + failed_count)*100:.1f}%" if (success_count + failed_count) > 0 else "0%"
        },
        "processed_documents": []
    }
    
    # æ”¶é›†æ¯ä¸ªå¤„ç†çš„æ–‡æ¡£ä¿¡æ¯
    for item in os.listdir(output_base_dir):
        item_path = os.path.join(output_base_dir, item)
        if os.path.isdir(item_path):
            doc_report_path = os.path.join(item_path, "processing_report.json")
            if os.path.exists(doc_report_path):
                try:
                    with open(doc_report_path, 'r', encoding='utf-8') as f:
                        doc_report = json.load(f)
                    report["processed_documents"].append({
                        "document_name": item,
                        "status": doc_report.get("processing_status", "unknown"),
                        "sections_found": doc_report.get("sections_found", {}),
                        "files_created": doc_report.get("files_created", {})
                    })
                except:
                    continue
    
    # ä¿å­˜æ‰¹å¤„ç†æŠ¥å‘Š
    batch_report_path = os.path.join(output_base_dir, "batch_processing_report.json")
    with open(batch_report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š æ‰¹å¤„ç†æŠ¥å‘Šå·²ä¿å­˜: {batch_report_path}")

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰¹é‡å¤„ç†PDFæ–‡ä»¶')
    parser.add_argument('input_dir', help='åŒ…å«PDFæ–‡ä»¶çš„è¾“å…¥ç›®å½•')
    parser.add_argument('-o', '--output', help='è¾“å‡ºåŸºç¡€ç›®å½•ï¼ˆé»˜è®¤ä¸ºinput_dir_outputï¼‰')
    parser.add_argument('--skip-existing', action='store_true', help='è·³è¿‡å·²å­˜åœ¨çš„è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(args.input_dir):
        print(f"âŒ é”™è¯¯: è¾“å…¥ç›®å½•ä¸å­˜åœ¨ - {args.input_dir}")
        return 1
    
    # ç¡®å®šè¾“å‡ºç›®å½•
    if args.output:
        output_base_dir = args.output
    else:
        dir_name = os.path.basename(os.path.abspath(args.input_dir))
        output_base_dir = f"{dir_name}_output"
    
    # å¼€å§‹æ‰¹é‡å¤„ç†
    batch_process_pdfs(args.input_dir, output_base_dir)
    return 0

if __name__ == "__main__":
    # å¯ä»¥ç›´æ¥è°ƒç”¨å‡½æ•°æˆ–ä½¿ç”¨å‘½ä»¤è¡Œ
    if len(sys.argv) > 1:
        exit(main())
    else:
        # ç¤ºä¾‹è°ƒç”¨
        INPUT_DIR = "/workspace/no1/test_do"  # åŒ…å«PDFæ–‡ä»¶çš„ç›®å½•
        OUTPUT_BASE_DIR = "/workspace/no1/output"  # è¾“å‡ºåŸºç¡€ç›®å½•
        
        # å¼€å§‹æ‰¹é‡å¤„ç†
        batch_process_pdfs(INPUT_DIR, OUTPUT_BASE_DIR)

import os
import glob
import shutil
from pdf_process_tools.main import main as process_pdf
from vector_utils import load_texts_from_output, build_faiss_index, model
import pickle
import numpy as np

def batch_process_pdfs(input_dir, output_base_dir):
    """
    æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹å†…æ‰€æœ‰PDFæ–‡ä»¶
    
    Args:
        input_dir: åŒ…å«PDFæ–‡ä»¶çš„è¾“å…¥ç›®å½•
        output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•ï¼Œæ¯ä¸ªPDFä¼šåœ¨æ­¤ç›®å½•ä¸‹åˆ›å»ºå¯¹åº”çš„æ–‡ä»¶å¤¹
    """
    # åˆ›å»ºè¾“å‡ºåŸºç¡€ç›®å½•
    os.makedirs(output_base_dir, exist_ok=True)
    
    # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
    pdf_files = glob.glob(os.path.join(input_dir, "*.pdf"))
    
    if not pdf_files:
        print(f"âš ï¸ åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶ï¼Œå¼€å§‹æ‰¹é‡å¤„ç†...")
    
    for i, pdf_path in enumerate(pdf_files, 1):
        pdf_filename = os.path.basename(pdf_path)
        pdf_name = os.path.splitext(pdf_filename)[0]  # å»æ‰.pdfæ‰©å±•å
        
        # ä¸ºæ¯ä¸ªPDFåˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
        doc_output_dir = os.path.join(output_base_dir, pdf_name)
        
        print(f"\nğŸ“„ [{i}/{len(pdf_files)}] æ­£åœ¨å¤„ç†: {pdf_filename}")
        print(f"   è¾“å‡ºç›®å½•: {doc_output_dir}")
        
        try:
            # å¦‚æœè¾“å‡ºç›®å½•å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†
            if os.path.exists(doc_output_dir) and os.path.exists(os.path.join(doc_output_dir, 'final_text.txt')):
                print(f"   â­ï¸ è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶: {pdf_filename}")
                continue
            
            # å¤„ç†PDF
            process_pdf(pdf_path, doc_output_dir)
            
            # å»ºç«‹å‘é‡ç´¢å¼•
            build_vector_index(doc_output_dir)
            
            print(f"   âœ… å®Œæˆå¤„ç†: {pdf_filename}")
            
        except Exception as e:
            print(f"   âŒ å¤„ç†å¤±è´¥: {pdf_filename}, é”™è¯¯: {e}")
            # æ¸…ç†å¤±è´¥çš„è¾“å‡ºç›®å½•
            if os.path.exists(doc_output_dir):
                shutil.rmtree(doc_output_dir, ignore_errors=True)
            continue
    
    print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_base_dir}")

def build_vector_index(doc_output_dir):
    """
    ä¸ºå•ä¸ªæ–‡æ¡£å»ºç«‹å‘é‡ç´¢å¼•
    
    Args:
        doc_output_dir: æ–‡æ¡£çš„è¾“å‡ºç›®å½•
    """
    print("   ğŸ” æ­£åœ¨å»ºç«‹å‘é‡ç´¢å¼•...")
    
    # åŠ è½½æ–‡æœ¬
    texts = load_texts_from_output(doc_output_dir)
    if not texts:
        print("   âš ï¸ æœªæ‰¾åˆ°æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡å‘é‡ç´¢å¼•å»ºç«‹")
        return
    
    # å»ºç«‹FAISSç´¢å¼•
    index, embeddings = build_faiss_index(texts)
    
    # ä¿å­˜ç´¢å¼•å’Œç›¸å…³æ•°æ®
    index_dir = os.path.join(doc_output_dir, "vector_index")
    os.makedirs(index_dir, exist_ok=True)
    
    # ä¿å­˜FAISSç´¢å¼•
    import faiss
    faiss.write_index(index, os.path.join(index_dir, "faiss.index"))
    
    # ä¿å­˜æ–‡æœ¬å’ŒåµŒå…¥å‘é‡
    with open(os.path.join(index_dir, "texts.pkl"), 'wb') as f:
        pickle.dump(texts, f)
    
    np.save(os.path.join(index_dir, "embeddings.npy"), embeddings)
    
    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        "num_texts": len(texts),
        "embedding_dim": embeddings.shape[1],
        "model_path": model.model_name_or_path if hasattr(model, 'model_name_or_path') else "unknown"
    }
    
    with open(os.path.join(index_dir, "metadata.json"), 'w', encoding='utf-8') as f:
        import json
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    print(f"   âœ… å‘é‡ç´¢å¼•å·²ä¿å­˜: {len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")

def load_vector_index(doc_output_dir):
    """
    åŠ è½½å·²ä¿å­˜çš„å‘é‡ç´¢å¼•
    
    Args:
        doc_output_dir: æ–‡æ¡£çš„è¾“å‡ºç›®å½•
    
    Returns:
        tuple: (index, texts, embeddings) æˆ– None
    """
    index_dir = os.path.join(doc_output_dir, "vector_index")
    
    if not os.path.exists(index_dir):
        return None
    
    try:
        import faiss
        # åŠ è½½FAISSç´¢å¼•
        index = faiss.read_index(os.path.join(index_dir, "faiss.index"))
        
        # åŠ è½½æ–‡æœ¬
        with open(os.path.join(index_dir, "texts.pkl"), 'rb') as f:
            texts = pickle.load(f)
        
        # åŠ è½½åµŒå…¥å‘é‡
        embeddings = np.load(os.path.join(index_dir, "embeddings.npy"))
        
        return index, texts, embeddings
    
    except Exception as e:
        print(f"âš ï¸ åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    INPUT_DIR = "/home/zhanggu/Project/tianchi/test"  # åŒ…å«PDFæ–‡ä»¶çš„ç›®å½•
    OUTPUT_BASE_DIR = "/home/zhanggu/Project/tianchi/no1/output"  # è¾“å‡ºåŸºç¡€ç›®å½•
    
    # å¼€å§‹æ‰¹é‡å¤„ç†
    batch_process_pdfs(INPUT_DIR, OUTPUT_BASE_DIR)

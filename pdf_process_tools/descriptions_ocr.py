import os
import re
import cv2
import numpy as np
import pdfplumber
from PIL import Image, ImageEnhance
from paddleocr import PPStructure, PaddleOCR
import fitz  # PyMuPDF

def detect_pdf_type(pdf_path, sample_pages=3):
    """
    æ£€æµ‹PDFç±»å‹ï¼šæ–‡æœ¬å‹ vs å›¾ç‰‡å‹
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        sample_pages: é‡‡æ ·é¡µæ•°è¿›è¡Œæ£€æµ‹
    
    Returns:
        'text': æ–‡æœ¬å‹PDF
        'image': å›¾ç‰‡å‹PDF
        'mixed': æ··åˆå‹PDF
    """
    print("ğŸ” æ­£åœ¨åˆ†æPDFç±»å‹...")
    
    text_pages = 0
    image_pages = 0
    total_checked = 0
    
    with pdfplumber.open(pdf_path) as pdf:
        # æ£€æŸ¥å‰å‡ é¡µæ¥åˆ¤æ–­ç±»å‹
        pages_to_check = min(sample_pages, len(pdf.pages))
        
        for i in range(pages_to_check):
            page = pdf.pages[i]
            text = page.extract_text()
            
            total_checked += 1
            
            # åˆ¤æ–­æ ‡å‡†ï¼š
            # 1. æ–‡æœ¬é•¿åº¦
            # 2. ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
            # 3. æœ‰æ•ˆæ–‡æœ¬è¡Œæ•°
            if text and len(text.strip()) > 50:
                # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
                chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', text))
                total_chars = len(re.sub(r'\s', '', text))
                
                if total_chars > 0:
                    chinese_ratio = chinese_chars / total_chars
                    # å¦‚æœä¸­æ–‡å­—ç¬¦å æ¯”>10%ï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆæ–‡æœ¬é¡µ
                    if chinese_ratio > 0.1 or total_chars > 200:
                        text_pages += 1
                        continue
            
            # å¦‚æœæ–‡æœ¬æå–å¤±è´¥æˆ–æ–‡æœ¬å¾ˆå°‘ï¼Œåˆ¤æ–­ä¸ºå›¾ç‰‡é¡µ
            image_pages += 1
    
    # åˆ¤æ–­é€»è¾‘
    text_ratio = text_pages / total_checked
    
    if text_ratio >= 0.8:
        pdf_type = 'text'
        print(f"ğŸ“„ æ£€æµ‹ç»“æœ: æ–‡æœ¬å‹PDF (æ–‡æœ¬é¡µ: {text_pages}/{total_checked})")
    elif text_ratio <= 0.2:
        pdf_type = 'image'
        print(f"ğŸ–¼ï¸  æ£€æµ‹ç»“æœ: å›¾ç‰‡å‹PDF (å›¾ç‰‡é¡µ: {image_pages}/{total_checked})")
    else:
        pdf_type = 'mixed'
        print(f"ğŸ“„ğŸ–¼ï¸  æ£€æµ‹ç»“æœ: æ··åˆå‹PDF (æ–‡æœ¬é¡µ: {text_pages}, å›¾ç‰‡é¡µ: {image_pages})")
    
    return pdf_type

def detect_content_area(page, margin_ratio=0.05):
    """
    æ”¹è¿›ç‰ˆåŠ¨æ€æ£€æµ‹é¡µé¢å†…å®¹åŒºåŸŸ,è‡ªåŠ¨è¿‡æ»¤é¡µçœ‰é¡µè„šå’Œé¡µç 
    
    Args:
        page: pdfplumberé¡µé¢å¯¹è±¡
        margin_ratio: åŸºç¡€è¾¹è·æ¯”ä¾‹ï¼ˆé»˜è®¤5%ï¼‰
    
    Returns:
        tuple: (x0, y0, x1, y1) å†…å®¹åŒºåŸŸåæ ‡
    """
    width, height = page.width, page.height
    
    # é»˜è®¤è¾¹è·
    default_margin_x = width * margin_ratio
    default_margin_y = height * margin_ratio
    
    try:
        # è·å–é¡µé¢ä¸Šæ‰€æœ‰æ–‡æœ¬å¯¹è±¡
        chars = page.chars
        
        if not chars:
            return (
                default_margin_x,
                default_margin_y,
                width - default_margin_x,
                height - default_margin_y
            )
            
        # æŒ‰yåæ ‡åˆ†ç»„ç»Ÿè®¡å­—ç¬¦åˆ†å¸ƒ
        y_groups = {}
        for char in chars:
            y = int(char['top'])
            if y not in y_groups:
                y_groups[y] = []
            y_groups[y].append(char)
        
        # é¡µç æ£€æµ‹ç‰¹å¾
        def is_page_number(char_group):
            # 1. é•¿åº¦ç‰¹å¾ï¼šé¡µç é€šå¸¸å¾ˆçŸ­
            if len(char_group) > 5:
                return False
                
            # 2. æ•°å­—ç‰¹å¾ï¼šé¡µç é€šå¸¸æ˜¯çº¯æ•°å­—
            text = ''.join(char['text'] for char in char_group)
            if not text.isdigit():
                return False
                
            # 3. ä½ç½®ç‰¹å¾ï¼šé€šå¸¸åœ¨é¡µé¢åº•éƒ¨å±…ä¸­
            avg_x = sum(char['x0'] for char in char_group) / len(char_group)
            center_zone = (width * 0.4, width * 0.6)  # ä¸­é—´åŒºåŸŸ
            if not (center_zone[0] < avg_x < center_zone[1]):
                return False
                
            return True

        # åˆ†æå‚ç›´æ–¹å‘çš„æ–‡æœ¬å¯†åº¦
        density_threshold = len(chars) / height * 0.3  # åŠ¨æ€å¯†åº¦é˜ˆå€¼
        
        # æ‰¾å‡ºé¡µçœ‰é¡µè„šçš„è¾¹ç•Œ
        header_bottom = 0
        footer_top = height
        
        sorted_y = sorted(y_groups.keys())
        last_valid_text_y = 0  # è®°å½•æœ€åä¸€ä¸ªæœ‰æ•ˆæ–‡æœ¬çš„ä½ç½®
        
        # æ£€æµ‹é¡µçœ‰
        for y in sorted_y:
            if len(y_groups[y]) < density_threshold and not is_page_number(y_groups[y]):
                header_bottom = y
            else:
                break

        # æ£€æµ‹é¡µè„š(è‡ªä¸‹è€Œä¸Š)
        for y in reversed(sorted_y):
            # å¦‚æœæ˜¯é¡µç ï¼Œè·³è¿‡è¿™ä¸€è¡Œ
            if is_page_number(y_groups[y]):
                continue
                
            if len(y_groups[y]) < density_threshold:
                footer_top = y
            else:
                last_valid_text_y = y
                break
        
        # è·å–æ°´å¹³æ–¹å‘è¾¹ç•Œ
        x_coords = [char['x0'] for char in chars] + [char['x1'] for char in chars]
        text_left = max(min(x_coords), default_margin_x)
        text_right = min(max(x_coords), width - default_margin_x)
        
        # æ·»åŠ å®‰å…¨è¾¹è·
        safe_margin = min(width, height) * 0.02
        content_x0 = max(0, text_left - safe_margin)
        content_y0 = max(header_bottom + safe_margin, default_margin_y)
        content_x1 = min(width, text_right + safe_margin)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæœ‰æ•ˆæ–‡æœ¬ä½ç½®æ¥è®¾ç½®åº•éƒ¨è¾¹ç•Œ
        if last_valid_text_y:
            content_y1 = min(last_valid_text_y + safe_margin, height - default_margin_y)
        else:
            content_y1 = min(footer_top - safe_margin, height - default_margin_y)
        
        # ç¡®ä¿åæ ‡æœ‰æ•ˆ
        content_x0 = max(0, content_x0)
        content_y0 = max(0, content_y0)
        content_x1 = min(width, content_x1)
        content_y1 = min(height, content_y1)
        
        return (content_x0, content_y0, content_x1, content_y1)
        
    except Exception as e:
        print(f"âš ï¸ å†…å®¹åŒºåŸŸæ£€æµ‹å¤±è´¥: {str(e)}")
        # å¦‚æœåˆ†æå¤±è´¥,ä½¿ç”¨é»˜è®¤è¾¹è·
        return (
            default_margin_x,
            default_margin_y,
            width - default_margin_x,
            height - default_margin_y
        )

def fix_chinese_soft_breaks(s):
    """ä¿®å¤ä¸­æ–‡æ¢è¡Œå¯¼è‡´çš„æ‹†è¯é—®é¢˜"""
    s = re.sub(r'-\s+', '', s)
    s = re.sub(r'(?<=[\u4e00-\u9fa5])\s+(?=[\u4e00-\u9fa5])', '', s)
    return s

def preprocess_image(img):
    """å›¾ç‰‡é¢„å¤„ç†ï¼Œæé«˜OCRè¯†åˆ«ç‡"""
    # è½¬æ¢ä¸ºç°åº¦å›¾
    if len(img.shape) == 3:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    else:
        gray = img
    
    # å›¾ç‰‡å¢å¼º
    # 1. é«˜æ–¯å»å™ª
    denoised = cv2.GaussianBlur(gray, (3, 3), 0)
    
    # 2. å¯¹æ¯”åº¦å¢å¼º
    pil_img = Image.fromarray(denoised)
    enhancer = ImageEnhance.Contrast(pil_img)
    enhanced = enhancer.enhance(1.5)  # å¢å¼ºå¯¹æ¯”åº¦
    
    # 3. é”åŒ–
    sharpness_enhancer = ImageEnhance.Sharpness(enhanced)
    sharpened = sharpness_enhancer.enhance(1.2)
    
    # è½¬æ¢å›opencvæ ¼å¼
    processed_img = np.array(sharpened)
    
    return processed_img

def process_text_with_paragraphs(text_lines):
    """ä¿®æ”¹ç‰ˆï¼šä»…å¤„ç†å¸¦æ‹¬å·çš„ç¼–å·"""
    text_output = []
    current_para = []
    merged_lines = []

    # ç¬¬ä¸€é˜¶æ®µï¼šæ™ºèƒ½åˆå¹¶è¢«æ‹†åˆ†çš„æ®µè½è¡Œï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    buffer = ""
    for line in (l.strip() for l in text_lines if l.strip()):
        # ä»…æ£€æµ‹å¸¦æ‹¬å·çš„ç¼–å·ä½œä¸ºæ–°æ®µè½èµ·å§‹
        if re.match(r'^[\[\(\ã€ï¼ˆï¼»]+.*\d+.*[\]ï¼‰\ã€‘ï¼½]*', line):  # ä¿®æ”¹è¡Œåˆå¹¶æ¡ä»¶
            if buffer:
                merged_lines.append(buffer)
                buffer = ""
        buffer = f"{buffer} {line}".strip() if buffer else line
    
    if buffer:
        merged_lines.append(buffer)

    # ç¬¬äºŒé˜¶æ®µï¼šç²¾ç¡®åŒ¹é…å¸¦æ‹¬å·çš„ç¼–å·
    para_pattern = re.compile(
        r'^('
        r'[\[\(\ã€ï¼ˆ\ï¼»]+\d+[\]ï¼‰\ã€‘ï¼½]*'  # æœ‰å‰æ‹¬å·
        r'|'                             # æˆ– 
        r'\d+[\]ï¼‰\ã€‘\ï¼½]+'               # æœ‰åæ‹¬å·
        r')'
        r'[\.\ã€‚]?'          # å¯é€‰ç»“æŸç¬¦
        r'\s*'               # åç»­ç©ºæ ¼
    )

    for line in merged_lines:
        

        # ä»…åŒ¹é…å¸¦æ‹¬å·çš„ç¼–å·
        match = para_pattern.match(line)
        if match:
            # è®¡ç®—ç¼–å·éƒ¨åˆ†é•¿åº¦ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
            match_len = match.end()
            remaining = line[match_len:].strip()

            # ä¿å­˜ä¸Šä¸€ä¸ªæ®µè½ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
            if current_para:
                joined = fix_chinese_soft_breaks(" ".join(current_para))
                text_output.append(joined)
                current_para = []
            
            if remaining:
                current_para.append(remaining)
        else:
            # æ— ç¼–å·å†…å®¹å¤„ç†ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
            if current_para:
                current_para.append(line)
            else:
                cleaned = fix_chinese_soft_breaks(line)
                text_output.append(cleaned)

    # å¤„ç†æœ€åä¸€æ®µï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
    if current_para:
        joined = fix_chinese_soft_breaks(" ".join(current_para))
        text_output.append(joined)

    return text_output

def extract_images_tables_with_ppstructure(pdf_path, page_num, current_id, img_counter, table_counter, img_dir, para_buffer):
    """ä½¿ç”¨PPStructureæå–å•é¡µçš„å›¾ç‰‡å’Œè¡¨æ ¼"""
    
    structure_engine = PPStructure(
        recovery=False,
        lang='ch',
        show_log=False
    )
    
    pdf_document = fitz.open(pdf_path)
    page = pdf_document[page_num]
    
    mat = fitz.Matrix(2.0, 2.0)
    pix = page.get_pixmap(matrix=mat)
    img_data = pix.tobytes("png")
    
    nparr = np.frombuffer(img_data, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    
    try:
        result = structure_engine(img)
        result.sort(key=lambda x: x['bbox'][1])
        
        for item in result:
            bbox = item['bbox']
            item_type = item['type']
            
            if item_type == 'figure':
                img_counter += 1
                x0, y0, x1, y1 = [int(coord) for coord in bbox]
                cropped_img = img[y0:y1, x0:x1]
                img_name = f"{current_id}_{img_counter}.png" if current_id else f"page{page_num+1}_img{img_counter}.png"
                img_path = os.path.join(img_dir, img_name)
                cv2.imwrite(img_path, cropped_img)
                para_buffer += f"\n[IMG_{img_counter}]"
                print(f"ğŸ“· æå–å›¾ç‰‡: {img_name}")
            
            elif item_type == 'table':
                table_counter += 1
                x0, y0, x1, y1 = [int(coord) for coord in bbox]
                cropped_table = img[y0:y1, x0:x1]
                table_name = f"{current_id}_table{table_counter}.png" if current_id else f"page{page_num+1}_table{table_counter}.png"
                table_path = os.path.join(img_dir, table_name)
                cv2.imwrite(table_path, cropped_table)
                para_buffer += f"\n[TABLE_{table_counter}]"
                print(f"ğŸ“Š æå–è¡¨æ ¼: {table_name}")
    
    except Exception as e:
        print(f"âš ï¸ PPStructureå¤„ç†ç¬¬{page_num+1}é¡µæ—¶å‡ºé”™: {str(e)}")
    
    finally:
        pdf_document.close()
    
    return img_counter, table_counter, para_buffer

def extract_text_pdf(pdf_path, output_dir):
    """
    æ–‡æœ¬å‹PDFå¤„ç†å™¨
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
    """
    print("ğŸ“„ ä½¿ç”¨æ–‡æœ¬å‹PDFå¤„ç†æ¨¡å¼...")
    
    os.makedirs(output_dir, exist_ok=True)
    img_dir = os.path.join(output_dir, "images")
    os.makedirs(img_dir, exist_ok=True)
    
    text_output = []
    img_counter = 0
    table_counter = 0
    all_text_lines = []  # æ”¶é›†æ‰€æœ‰æ–‡æœ¬è¡Œ
    
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            print(f"å¤„ç†ç¬¬ {page_num + 1}/{len(pdf.pages)} é¡µ")
            
            # åŠ¨æ€æ£€æµ‹å†…å®¹åŒºåŸŸ
            content_area = detect_content_area(page)
            crop = page.within_bbox(content_area)
            text = crop.extract_text()
            
            if not text:
                print(f"  âš ï¸ ç¬¬{page_num+1}é¡µæœªæå–åˆ°æ–‡æœ¬")
                continue
            
            # æŒ‰è¡Œåˆ†å‰²å¹¶æ¸…ç†
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            all_text_lines.extend(lines)
            
            print(f"  ğŸ“ æå–äº† {len(lines)} è¡Œæ–‡æœ¬")
            
            # PPStructureå¢å¼ºå›¾ç‰‡è¡¨æ ¼æå–
            try:
                temp_para_buffer = ""
                img_counter, table_counter, _ = extract_images_tables_with_ppstructure(
                    pdf_path, page_num, None, img_counter, table_counter, img_dir, temp_para_buffer
                )
            except Exception as e:
                print(f"âš ï¸ é¡µé¢{page_num+1}çš„PPStructureå¤„ç†å¤±è´¥: {str(e)}")
    
    # æ™ºèƒ½å¤„ç†æ–‡æœ¬æ®µè½
    print(f"ğŸ”„ å¤„ç†æå–çš„æ–‡æœ¬ï¼Œå…± {len(all_text_lines)} è¡Œ")
    
    # ä½¿ç”¨æ”¹è¿›çš„æ®µè½åˆ†å‰²é€»è¾‘
    text_output = smart_paragraph_split(all_text_lines)
    
    # ä¿å­˜æ–‡æœ¬æ–‡ä»¶
    text_file = os.path.join(output_dir, "descriptions.txt")
    with open(text_file, "w", encoding="utf-8") as f:
        f.write("\n\n".join(text_output))
    
    # ç”ŸæˆJSONæ–‡ä»¶
    json_file = os.path.join(output_dir, "descriptions.json")
    try:
        convert_text_to_json(text_file, json_file)
        print(f"âœ… JSONæ–‡ä»¶å·²ç”Ÿæˆ: {json_file}")
    except Exception as e:
        print(f"âš ï¸ JSONè½¬æ¢å¤±è´¥: {str(e)}")
    
    print(f"âœ… æ–‡æœ¬æå–å®Œæˆï¼Œå…± {len(text_output)} ä¸ªæ®µè½")
    return len(text_output), img_counter, table_counter

import re

def smart_paragraph_split(text_lines):
    paragraphs = []
    current_paragraph = []

    # å¸¸è§ç¼–å·æ ¼å¼
    number_patterns = [
        r'^\d+[\.\ï¼]',                    # 1. 2. 3.
        r'^[\[\(ï¼ˆ\ï¼ˆ]+\d+[\]\)ï¼‰\ï¼‰]+',    # [1] (1) ï¼ˆ1ï¼‰
        r'^\d+[\)ï¼‰]',                    # 1) 2)
    ]

    # ç»“æ„æ€§æ ‡é¢˜å…³é”®è¯
    SECTION_TITLES = {
        "æŠ€æœ¯é¢†åŸŸ": ["æŠ€æœ¯é¢†åŸŸ"],
        "èƒŒæ™¯æŠ€æœ¯": ["èƒŒæ™¯æŠ€æœ¯"],
        "å‘æ˜å†…å®¹": ["å‘æ˜å†…å®¹", "å®ç”¨æ–°å‹å†…å®¹"],
        "é™„å›¾è¯´æ˜": ["é™„å›¾è¯´æ˜"],
        "å…·ä½“å®æ–½æ–¹å¼": ["å…·ä½“å®æ–½æ–¹å¼"]
    }

    def is_numbered_line(line):
        for pattern in number_patterns:
            if re.match(pattern, line):
                return True
        return False

    def is_title_like(line):
        return len(line) < 50 and (line.isupper() or line.endswith('ï¼š') or line.endswith(':'))

    def is_section_title(line):
        stripped = line.strip().replace(" ", "")
        for section, aliases in SECTION_TITLES.items():
            for alias in aliases:
                if stripped == alias:
                    return section
        return None

    def fix_chinese_soft_breaks(s):
        s = re.sub(r'-\s+', '', s)
        s = re.sub(r'(?<=[\u4e00-\u9fa5])\s+(?=[\u4e00-\u9fa5])', '', s)
        return s

    for i, line in enumerate(text_lines):
        line = line.strip()
        if not line:
            continue

        section = is_section_title(line)
        if section:
            # ä¿å­˜å½“å‰æ®µè½
            if current_paragraph:
                paragraph_text = fix_chinese_soft_breaks(" ".join(current_paragraph))
                if paragraph_text.strip():
                    paragraphs.append(paragraph_text)
                current_paragraph = []

            # å°æ ‡é¢˜ç‹¬ç«‹æˆæ®µ
            paragraphs.append(section)
            continue

        is_new_paragraph = False
        if is_numbered_line(line):
            is_new_paragraph = True
        elif is_title_like(line):
            is_new_paragraph = True
        elif i > 0 and len(line) < 20 and not line.endswith(('ï¼Œ', 'ã€‚', 'ï¼›', 'ï¼š', ',', '.', ';', ':')):
            is_new_paragraph = True

        if is_new_paragraph and current_paragraph:
            paragraph_text = fix_chinese_soft_breaks(" ".join(current_paragraph))
            if paragraph_text.strip():
                paragraphs.append(paragraph_text)
            current_paragraph = []

        current_paragraph.append(line)

    if current_paragraph:
        paragraph_text = fix_chinese_soft_breaks(" ".join(current_paragraph))
        if paragraph_text.strip():
            paragraphs.append(paragraph_text)

    # è¿‡æ»¤è¿‡çŸ­çš„æ®µè½ï¼ˆå¯èƒ½æ˜¯å™ªå£°ï¼‰
    filtered_paragraphs = [p for p in paragraphs if len(p.strip()) > 1]

    return filtered_paragraphs

def extract_image_pdf(pdf_path, output_dir):
    """
    å›¾ç‰‡å‹PDFå¤„ç†å™¨
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
    """
    print(f"ğŸ–¼ï¸ ä½¿ç”¨å›¾ç‰‡å‹PDFå¤„ç†æ¨¡å¼: {os.path.basename(pdf_path)}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    img_dir = os.path.join(output_dir, "images")
    os.makedirs(img_dir, exist_ok=True)
    
    # åˆå§‹åŒ–OCRå¼•æ“
    print("ğŸ”§ åˆå§‹åŒ–OCRå¼•æ“...")
    ocr_engine = PaddleOCR(
        use_angle_cls=True,
        lang='ch',
        use_gpu=False,
        show_log=False
    )
    
    # åˆå§‹åŒ–ç»“æ„åˆ†æå¼•æ“
    print("ğŸ”§ åˆå§‹åŒ–ç»“æ„åˆ†æå¼•æ“...")
    structure_engine = PPStructure(
        recovery=True,
        lang='ch',
        show_log=False
    )
    
    # æ‰“å¼€PDF
    pdf_document = fitz.open(pdf_path)
    total_pages = pdf_document.page_count
    
    all_text_lines = []  # æ”¶é›†æ‰€æœ‰æ–‡æœ¬è¡Œ
    img_counter = 0
    table_counter = 0
    
    print(f"ğŸ“– PDFæ€»é¡µæ•°: {total_pages}")
    
    for page_num in range(total_pages):
        print(f"\n--- ğŸ“„ å¤„ç†ç¬¬ {page_num + 1}/{total_pages} é¡µ ---")
        
        page = pdf_document[page_num]
        
        # è½¬æ¢ä¸ºé«˜åˆ†è¾¨ç‡å›¾ç‰‡
        mat = fitz.Matrix(3.0, 3.0)
        pix = page.get_pixmap(matrix=mat)
        img_data = pix.tobytes("png")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        nparr = np.frombuffer(img_data, np.uint8)
        original_img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        # å›¾ç‰‡é¢„å¤„ç†
        processed_img = preprocess_image(original_img)
        
        page_text_lines = []
        structure_success = False
        
        # æ–¹æ³•1: å°è¯•ä½¿ç”¨PPStructureè¿›è¡Œç»“æ„åŒ–åˆ†æ
        print("ğŸ”¬ å°è¯•PPStructureç»“æ„åŒ–åˆ†æ...")
        try:
            structure_result = structure_engine(original_img)
            
            if structure_result:
                # æŒ‰yåæ ‡æ’åºï¼Œä¿è¯é˜…è¯»é¡ºåº
                structure_result.sort(key=lambda x: x['bbox'][1])
                
                for item in structure_result:
                    bbox = item['bbox']
                    item_type = item['type']
                    
                    if item_type == 'text':
                        # å¤„ç†æ–‡æœ¬åŒºåŸŸ
                        text_content = item.get('res', [])
                        if isinstance(text_content, list) and text_content:
                            for text_item in text_content:
                                if isinstance(text_item, dict) and 'text' in text_item:
                                    confidence = text_item.get('confidence', 0)
                                    if confidence > 0.5:
                                        page_text_lines.append(text_item['text'])
                                        structure_success = True
                                elif isinstance(text_item, str):
                                    page_text_lines.append(text_item)
                                    structure_success = True
                    
                    elif item_type == 'figure':
                        # å¤„ç†å›¾ç‰‡
                        img_counter += 1
                        x0, y0, x1, y1 = [int(coord) for coord in bbox]
                        
                        h, w = original_img.shape[:2]
                        x0, x1 = max(0, min(x0, w)), max(0, min(x1, w))
                        y0, y1 = max(0, min(y0, h)), max(0, min(y1, h))
                        
                        if x1 > x0 and y1 > y0:
                            cropped_img = original_img[y0:y1, x0:x1]
                            img_name = f"page{page_num+1}_img{img_counter}.png"
                            img_path = os.path.join(img_dir, img_name)
                            cv2.imwrite(img_path, cropped_img)
                            page_text_lines.append(f"[IMG_{img_counter}]")
                            print(f"  ğŸ“· æå–å›¾ç‰‡: {img_name}")
                    
                    elif item_type == 'table':
                        # å¤„ç†è¡¨æ ¼
                        table_counter += 1
                        x0, y0, x1, y1 = [int(coord) for coord in bbox]
                        
                        h, w = original_img.shape[:2]
                        x0, x1 = max(0, min(x0, w)), max(0, min(x1, w))
                        y0, y1 = max(0, min(y0, h)), max(0, min(y1, h))
                        
                        if x1 > x0 and y1 > y0:
                            cropped_table = original_img[y0:y1, x0:x1]
                            table_name = f"page{page_num+1}_table{table_counter}.png"
                            table_path = os.path.join(img_dir, table_name)
                            cv2.imwrite(table_path, cropped_table)
                            page_text_lines.append(f"[TABLE_{table_counter}]")
                            print(f"  ğŸ“Š æå–è¡¨æ ¼: {table_name}")
        
        except Exception as e:
            print(f"âš ï¸ PPStructureåˆ†æå¤±è´¥: {str(e)}")
        
        # æ–¹æ³•2: å¦‚æœPPStructureæ²¡æœ‰æˆåŠŸæå–æ–‡æœ¬ï¼Œä½¿ç”¨çº¯OCR
        if not structure_success:
            print("ğŸ”¤ ä½¿ç”¨çº¯OCRæ¨¡å¼...")
            try:
                # åœ¨OCRä¹‹å‰æ·»åŠ å†…å®¹åŒºåŸŸæ£€æµ‹å’Œè£å‰ª
                page = pdf_document[page_num]
                content_area = detect_content_area(page)
                x0, y0, x1, y1 = [int(coord) for coord in content_area]

                # è£å‰ªå›¾ç‰‡åˆ°å†…å®¹åŒºåŸŸ
                h, w = original_img.shape[:2]
                content_img = original_img[
                    int(y0 * h / page.height):int(y1 * h / page.height),
                    int(x0 * w / page.width):int(x1 * w / page.width)
                ]

                # å¯¹è£å‰ªåçš„å›¾ç‰‡è¿›è¡ŒOCRå¤„ç†
                processed_img = preprocess_image(content_img)
                ocr_result = ocr_engine.ocr(processed_img, cls=True)
                
                if ocr_result and ocr_result[0]:
                    # æŒ‰ç…§yåæ ‡æ’åºOCRç»“æœ
                    ocr_lines = sorted(ocr_result[0], key=lambda x: x[0][0][1])
                    
                    for line in ocr_lines:
                        text = line[1][0]
                        confidence = line[1][1]
                        
                        # ç½®ä¿¡åº¦è¿‡æ»¤
                        if confidence > 0.6:
                            page_text_lines.append(text)
            
            except Exception as e:
                print(f"âŒ OCRå¤„ç†å¤±è´¥: {str(e)}")
        
        # å°†é¡µé¢æ–‡æœ¬æ·»åŠ åˆ°æ€»æ–‡æœ¬ä¸­
        if page_text_lines:
            all_text_lines.extend(page_text_lines)
        else:
            print("  âš ï¸ æœ¬é¡µæœªæå–åˆ°æ–‡æœ¬")
    
    pdf_document.close()
    
    # æ™ºèƒ½å¤„ç†æ–‡æœ¬æ®µè½
    print(f"\nğŸ”„ å¤„ç†æå–çš„æ–‡æœ¬ï¼Œå…± {len(all_text_lines)} è¡Œ")
    text_output = smart_paragraph_split(all_text_lines)
    
    # ä¿å­˜æ–‡æœ¬æ–‡ä»¶
    text_file = os.path.join(output_dir, "descriptions.txt")
    with open(text_file, "w", encoding="utf-8") as f:
        f.write("\n\n".join(text_output))
    
    # ç”ŸæˆJSONæ–‡ä»¶
    json_file = os.path.join(output_dir, "descriptions.json")
    try:
        convert_text_to_json(text_file, json_file)
        print(f"âœ… JSONæ–‡ä»¶å·²ç”Ÿæˆ: {json_file}")
    except Exception as e:
        print(f"âš ï¸ JSONè½¬æ¢å¤±è´¥: {str(e)}")
    
    return len(text_output), img_counter, table_counter

def convert_text_to_json(text_file, json_file):
    """
    å°†æå–çš„æ–‡æœ¬è½¬æ¢ä¸ºç»“æ„åŒ–JSONæ ¼å¼
    
    Args:
        text_file: è¾“å…¥çš„txtæ–‡ä»¶è·¯å¾„
        json_file: è¾“å‡ºçš„jsonæ–‡ä»¶è·¯å¾„
    """
    import json
    
    # å®šä¹‰æ‰€æœ‰å¯èƒ½çš„éƒ¨åˆ†æ ‡é¢˜
    SECTIONS = {
        "æŠ€æœ¯é¢†åŸŸ": ["æŠ€æœ¯é¢†åŸŸ"],
        "èƒŒæ™¯æŠ€æœ¯": ["èƒŒæ™¯æŠ€æœ¯"],
        "å‘æ˜å†…å®¹": ["å‘æ˜å†…å®¹", "å®ç”¨æ–°å‹å†…å®¹"],
        "é™„å›¾è¯´æ˜": ["é™„å›¾è¯´æ˜"],
        "å…·ä½“å®æ–½æ–¹å¼": ["å…·ä½“å®æ–½æ–¹å¼", "å…·ä½“å®æ–½ä¾‹"]
    }
    
    # åˆå§‹åŒ–ç»“æ„
    result = {
        "é¢˜ç›®": [],
        "æŠ€æœ¯é¢†åŸŸ": [],
        "èƒŒæ™¯æŠ€æœ¯": [],
        "å‘æ˜å†…å®¹/å®ç”¨æ–°å‹å†…å®¹": [],
        "é™„å›¾è¯´æ˜": [],
        "å…·ä½“å®æ–½æ–¹å¼": []
    }
    
    # è¯»å–æ–‡æœ¬æ–‡ä»¶
    with open(text_file, 'r', encoding='utf-8') as f:
        lines = [line.strip() for line in f.readlines() if line.strip()]
    
    # æå–é¢˜ç›® (ç¬¬ä¸€è¡Œ)
    if lines:
        result["é¢˜ç›®"].append(lines[0])
        lines = lines[1:]  # ç§»é™¤é¢˜ç›®è¡Œ
    
    # æŸ¥æ‰¾å„ä¸ªéƒ¨åˆ†çš„èµ·å§‹ä½ç½®
    section_positions = {}
    current_section = None
    
    for i, line in enumerate(lines):
        stripped_line = line.strip().replace(" ", "")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å°æ ‡é¢˜
        for section, aliases in SECTIONS.items():
            if stripped_line in aliases:
                section_positions[i] = section
                current_section = section
                break
    
    # æŒ‰é¡ºåºæå–å„ä¸ªéƒ¨åˆ†çš„å†…å®¹
    if section_positions:
        # å°†ä½ç½®ä¿¡æ¯è½¬æ¢ä¸ºæ’åºåçš„åˆ—è¡¨
        sorted_positions = sorted(section_positions.items())
        
        # å¤„ç†å„ä¸ªéƒ¨åˆ†
        for i, (pos, section) in enumerate(sorted_positions):
            start = pos + 1  # è·³è¿‡æ ‡é¢˜è¡Œ
            
            # ç¡®å®šç»“æŸä½ç½®
            if i < len(sorted_positions) - 1:
                end = sorted_positions[i + 1][0]
            else:
                end = len(lines)
            
            # æå–å½“å‰éƒ¨åˆ†çš„å†…å®¹
            content = lines[start:end]
            result[section].extend([line for line in content if line.strip()])
    
    # å†™å…¥JSONæ–‡ä»¶
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    return result

# åœ¨ smart_extract_pdf å‡½æ•°çš„æœ€åæ·»åŠ  JSON è½¬æ¢
def smart_extract_pdf(pdf_path, output_dir):
    """æ™ºèƒ½PDFæå– - è‡ªåŠ¨åˆ¤æ–­ç±»å‹å¹¶é€‰æ‹©åˆé€‚çš„å¤„ç†æ–¹æ³•"""
    
    print(f"ğŸš€ å¼€å§‹æ™ºèƒ½å¤„ç†PDF: {os.path.basename(pdf_path)}")
    
    # ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹PDFç±»å‹
    pdf_type = detect_pdf_type(pdf_path)
    
    # ç¬¬äºŒæ­¥ï¼šé€‰æ‹©å¯¹åº”çš„å¤„ç†æ–¹æ³•
    if pdf_type == 'text':
        paragraphs, images, tables = extract_text_pdf(pdf_path, output_dir)  # ä½¿ç”¨ä¿®å¤ç‰ˆ
    elif pdf_type == 'image':
        paragraphs, images, tables = extract_image_pdf(pdf_path, output_dir)
    else:  # mixed
        print("ğŸ“„ğŸ–¼ï¸ æ··åˆå‹PDFï¼Œä½¿ç”¨ä¿®å¤ç‰ˆæ–‡æœ¬æ¨¡å¼å¤„ç†ï¼ˆä¸»è¦é€»è¾‘ï¼‰+ OCRè¡¥å……")
        # æ··åˆå‹ä½¿ç”¨ä¿®å¤ç‰ˆæ–‡æœ¬æ¨¡å¼ï¼Œåç»­å¯ä»¥ä¼˜åŒ–ä¸ºé€é¡µåˆ¤æ–­
        paragraphs, images, tables = extract_text_pdf(pdf_path, output_dir)
        
        print(f"\nâœ… å¤„ç†å®Œæˆï¼")
        print(f"   ğŸ“Š PDFç±»å‹: {pdf_type}")
        print(f"   ğŸ“„ æ®µè½æ•°: {paragraphs}")
        print(f"   ğŸ“· å›¾ç‰‡æ•°: {images}")
        
        # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„æ–‡ä»¶å
        text_file = os.path.join(output_dir, "descriptions.txt")  # ä¿®æ”¹è¿™é‡Œ
        json_file = os.path.join(output_dir, "descriptions.json") # ä¿®æ”¹è¿™é‡Œ
    
    try:
        json_content = convert_text_to_json(text_file, json_file)
        print(f"   ğŸ“‹ JSONæ–‡ä»¶å·²ç”Ÿæˆ: {json_file}")
    except Exception as e:
        print(f"âš ï¸ JSONè½¬æ¢å¤±è´¥: {str(e)}")
    
    return {
        'pdf_type': pdf_type,
        'paragraphs': paragraphs,
        'images': images,
        'tables': tables,
        'json_file': json_file if 'json_file' in locals() else None
    }

# ç”¨æ³•ç¤ºä¾‹
if __name__ == "__main__":
    pdf_path = r"/workspace/project/output/descriptions.pdf"
    output_dir = "output_descriptions"
    
    try:
        result = smart_extract_pdf(pdf_path, output_dir)
        
        print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼è¾“å‡ºæ–‡ä»¶ï¼š")
        print(f"   ğŸ“„ descriptions.txt - ç»“æ„åŒ–æ–‡æœ¬ ({result['paragraphs']} æ®µè½)")  # ä¿®æ”¹è¿™é‡Œ
        print(f"   ğŸ“ images/ - {result['images']} ä¸ªå›¾ç‰‡ + {result['tables']} ä¸ªè¡¨æ ¼")
        print(f"   ğŸ” PDFç±»å‹: {result['pdf_type']}")
        
        # å¦‚æœæœ‰JSONæ–‡ä»¶ï¼Œæ˜¾ç¤ºè·¯å¾„
        if result.get('json_file'):
            print(f"   ğŸ“‹ JSONæ–‡ä»¶: {result['json_file']}")
        
    except ImportError as e:
        print(f"âŒ ä¾èµ–åº“ç¼ºå¤±: {str(e)}")
        print("ğŸ’¡ è¯·å®‰è£…: pip install paddlepaddle paddleocr PyMuPDF opencv-python pdfplumber pillow")
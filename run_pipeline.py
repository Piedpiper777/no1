import os
import json
import re
import glob
from vector_utils import load_texts_from_output, retrieve
from llm_utils import call_llm_with_context

DATA_PATH = "data.json"
OUTPUT_DIR = "output"

def extract_page_numbers(question):
    """ä»é—®é¢˜ä¸­æå–é¡µç ä¿¡æ¯"""
    page_patterns = [
        r'ç¬¬(\d+)é¡µ',
        r'ç¬¬(\d+)é¡µçš„',
        r'é¡µç (\d+)',
        r'(\d+)é¡µ'
    ]
    
    pages = []
    for pattern in page_patterns:
        matches = re.findall(pattern, question)
        pages.extend([int(match) for match in matches])
    
    return list(set(pages))  # å»é‡

def find_figure_files(doc_output_dir, pages):
    """æ ¹æ®é¡µç æŸ¥æ‰¾å¯¹åº”çš„å›¾è¡¨æ–‡ä»¶"""
    figure_files = []
    
    if not pages:
        return figure_files
    
    for page_num in pages:
        # æŸ¥æ‰¾åŒ…å«æŒ‡å®šé¡µç çš„å›¾è¡¨æ–‡ä»¶
        pattern = os.path.join(doc_output_dir, f"*page{page_num}*.png")
        files = glob.glob(pattern)
        figure_files.extend(files)
    
    return figure_files

def process_single_item(item):
    """å¤„ç†å•ä¸ªæ•°æ®é¡¹"""
    pdf_filename = item['document']
    
    # è·å–å¯¹åº”æ–‡æ¡£çš„è¾“å‡ºç›®å½•ï¼ˆå·²é¢„å¤„ç†å®Œæˆï¼‰
    doc_output_dir = os.path.join(OUTPUT_DIR, pdf_filename.replace('.pdf', ''))
    
    if not os.path.exists(doc_output_dir):
        print(f"[é”™è¯¯] æœªæ‰¾åˆ°é¢„å¤„ç†ç»“æœ: {doc_output_dir}")
        return None

    print(f"\nğŸ“„ æ­£åœ¨å¤„ç†: {pdf_filename}")

    # Step 1: åŠ è½½å·²æ„å»ºçš„å‘é‡æ•°æ®åº“å’Œæ–‡æœ¬
    texts = load_texts_from_output(doc_output_dir)
    if not texts:
        print(f"[è­¦å‘Š] æ— æ³•åŠ è½½æ–‡æœ¬æ•°æ®: {pdf_filename}")
        return None

    # Step 2: åŸºäºé—®é¢˜æ£€ç´¢ç›¸å…³æ–‡æœ¬
    question = item["question"]
    retrieved_texts = retrieve(texts, question, top_k=5)

    # Step 3: æ£€æŸ¥é—®é¢˜ä¸­æ˜¯å¦åŒ…å«é¡µç ä¿¡æ¯ï¼Œå¦‚æœ‰åˆ™æ£€ç´¢å¯¹åº”å›¾ç‰‡
    pages = extract_page_numbers(question)
    figure_files = []
    
    if pages:
        print(f"ğŸ” æ£€æµ‹åˆ°é¡µç : {pages}")
        figure_files = find_figure_files(doc_output_dir, pages)
        if figure_files:
            print(f"ğŸ“Š æ‰¾åˆ°å›¾è¡¨æ–‡ä»¶: {[os.path.basename(f) for f in figure_files]}")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°é¡µç  {pages} å¯¹åº”çš„å›¾è¡¨æ–‡ä»¶")

    # Step 4: è°ƒç”¨å¤§æ¨¡å‹
    response = call_llm_with_context(
        question=question,
        options=item["options"],
        retrieved_texts=retrieved_texts,
        figure_files=figure_files
    )
    
    # Step 5: æ‰“å°ç»“æœ
    print_results(item, retrieved_texts, figure_files, response)
    
    return response

def print_results(item, retrieved_texts, figure_files, response):
    """æ‰“å°å¤„ç†ç»“æœ"""
    print("ğŸ¯ é—®é¢˜:", item["question"])
    for option in item["options"]:
        print(f"   {option}")
    print("âœ… æ­£ç¡®ç­”æ¡ˆ:", item["answer"])
    
    print("ğŸ” æ£€ç´¢åˆ°çš„æ–‡æœ¬:")
    for i, text in enumerate(retrieved_texts, 1):
        print(f"   {i}. {text[:100]}..." if len(text) > 100 else f"   {i}. {text}")
    
    if figure_files:
        print("ğŸ“Š ç›¸å…³å›¾è¡¨:")
        for fig in figure_files:
            print(f"   - {os.path.basename(fig)}")
    
    print("ğŸ¤– æ¨¡å‹å›ç­”:", response)
    print("-" * 80)

def run_pipeline():
    """è¿è¡Œå®Œæ•´æµæ°´çº¿"""
    with open(DATA_PATH, 'r', encoding='utf-8') as f:
        dataset = json.load(f)

    results = []
    for item in dataset:
        result = process_single_item(item)
        if result:
            results.append({
                'id': item.get('id'),
                'question': item['question'],
                'options': item['options'],
                'model_answer': result, 
                'document': item['document']
            })
    
    # ä¿å­˜ç»“æœ
    with open('pipeline_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° pipeline_results.json")

if __name__ == "__main__":
    run_pipeline()
